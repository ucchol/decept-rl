from __future__ import annotations

import argparse
import numpy as np
from stable_baselines3 import PPO

from src.envs.attack_graph_env import AttackGraphEnv, EnvConfig
ATTACKERS = ["shortest", "stealthy", "random"]
def eval_on_attacker(model, attacker_policy: str, episodes: int = 1000):
    env = AttackGraphEnv(EnvConfig(n_nodes=12, horizon=30, seed=123, attacker_policy=attacker_policy))

    outcomes = {"goal_reached": 0, "detected_on_decoy": 0, "horizon": 0, "other": 0}
    returns = []

    for _ in range(episodes):
        obs, _ = env.reset()
        total = 0.0
        done = False

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, r, term, trunc, info = env.step(int(action))
            total += float(r)
            done = term or trunc

        evt = info.get("event", "other")
        outcomes[evt] = outcomes.get(evt, 0) + 1
        returns.append(total)

    returns = np.asarray(returns, dtype=np.float64)
    total_eps = sum(outcomes.values())

    print(f"\n=== PPO evaluated on attacker: {attacker_policy} ===")
    print("Episodes:", total_eps)
    print("Mean return:", float(returns.mean()))
    print("Std return:", float(returns.std()))
    print("Outcome rates:")
    for k, v in outcomes.items():
        print(f"  {k:18s}: {v:4d} ({v/total_eps:.2%})")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model", required=True)
    ap.add_argument("--episodes", type=int, default=1000)
    args = ap.parse_args()

    model = PPO.load(args.model)

    eval_on_attacker(model, "shortest", episodes=args.episodes)
    eval_on_attacker(model, "random", episodes=args.episodes)


if __name__ == "__main__":
    main()


